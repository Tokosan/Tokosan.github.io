{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tokosan/Tokosan.github.io/blob/master/CC6204_Tarea_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CC6204 - Deep Learning**\n",
        "\n",
        "## **Predicción de riesgo de diabetes en etapas tempranas**\n",
        "\n",
        "En esta tarea vas a desarrollar un modelo de aprendizaje que sea capaz de  determinar si una persona tiene riesgo de paceder diabetes en un futuro. Los datos han sido coleccionados a través de encuestas a pacientes en el Sylhet Diabetes Hospital en Bangladesh. Los datos han sido curados y verificados por profesionales de la salud, por lo que son confiables para crear un modelo de aprendizaje.\n",
        "\n",
        "Primero vamos a importar los paquetes necesarios para trabajar en estos datos."
      ],
      "metadata": {
        "id": "O_emFGVipW5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "05yDKKyhzMMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Descarga de archivo de datos**\n",
        "Los datos están disponibles en un archivo CSV que contiene 520 muestras de 17 atributos. Los atributos son:\n",
        "\n",
        "*   Age: numérico\n",
        "*   Género: \\[Male, Female\\]\n",
        "*   Polyuria\n",
        "*   Polydipsia\n",
        "*   sudden weight loss\n",
        "*   weakness\n",
        "*   Polyphagia\n",
        "*   Genital thrush\n",
        "*   visual blurring\n",
        "*   Itching\n",
        "*   Irritability\n",
        "*   delayed healing\n",
        "*   partial paresis\n",
        "*   muscle stiffness\n",
        "*   Alopecia\n",
        "*   Obesity\n",
        "*   Class: \\[Positive, Negative\\]\n",
        "\n",
        "Todos los atributos descritos sin valores tienen el conjunto \\[Yes, No\\].\n",
        "\n",
        "En la siguiente celda de código, descargamos el archivo y lo leemos con Pandas. Finalmente, visualizamos algunos datos del conjunto.\n",
        "\n"
      ],
      "metadata": {
        "id": "VP3RKsqXr9Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00529/diabetes_data_upload.csv\n",
        "dataset_path = 'diabetes_data_upload.csv'"
      ],
      "metadata": {
        "id": "mObgujPHmuBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('diabetes_data_upload.csv') as inp, open('temp.csv', 'w') as out:\n",
        "    reader = csv.reader(inp)\n",
        "    writer = csv.writer(out, delimiter=',')\n",
        "    #No need to use `insert(), `append()` simply use `+` to concatenate two lists.\n",
        "    writer.writerow(['ID'] + next(reader))\n",
        "    #Iterate over enumerate object of reader and pass the starting index as 1.\n",
        "    writer.writerows([i] + row for i, row in enumerate(reader, 1))"
      ],
      "metadata": {
        "id": "9WjIFnaAvsYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['Age','Gender','Polyuria','Polydipsia','sudden weight loss',\n",
        "                'weakness', 'Polyphagia', 'Genital thrush', 'visual blurring','Itching', 'Irritability', 'delayed healing',\n",
        "                'partial paresis', 'muscle stiffness', 'Alopecia', 'Obesity', 'class']\n",
        "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\",\", skipinitialspace=True, header=1)\n",
        "\n",
        "dataset = raw_dataset.copy()\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "eqQvbIZuzPsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-procesamiento del conjunto de datos**\n",
        "El conjunto de datos es variado. El atributo \"Age\" es el único atributo numérico. Todos los demás atributos son nominales. Para procesar los datos nominales en una red neuronal, es mejor convertirlos a una representación numérica. En el siguiente ejemplo transformamos el atributo \"Gender\" con valores nominales \"Female\" y \"Male\" a valores 1.0 y 0.0, respectivamente.\n",
        "\n",
        "Del mismo modo, cambiamos los valores nominales de todos los atributos a valores 0.0 y 1.0."
      ],
      "metadata": {
        "id": "KtRzLcq_3ecc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gender = dataset.pop('Gender')\n",
        "dataset['gender'] = (gender == 'Female')*1.0\n",
        "\n",
        "column_class = dataset.pop('class')\n",
        "dataset['class'] = (column_class=='Positive')*1.0\n",
        "\n",
        "for column in column_names:\n",
        "  if column not in ['Gender', 'class', 'Age']:\n",
        "    column_class = dataset.pop(column)\n",
        "    dataset[column] = (column_class=='Yes')*1.0\n",
        "\n",
        "# Prueba mostrando parte de la data para ver si tu conversión se hizo correctamente\n",
        "dataset.tail()\n"
      ],
      "metadata": {
        "id": "5j7aHuwF1tsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora normalizamos el atributo \"Age\" y dividimos la data en conjunto de entrenamiento y conjunto de test. Esta división es siempre necesaria para poder probar si tu modelo de aprendizaje ha aprendido a generalizar con datos que no pertenecen al conjunto de entrenamiento."
      ],
      "metadata": {
        "id": "rZ01e3pV5JhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_age = dataset[\"Age\"].max()\n",
        "dataset[\"Age\"] = dataset[\"Age\"] / max_age\n",
        "dataset.tail()\n",
        "\n",
        "\n",
        "#80% de datos para train y 20% de datos para test\n",
        "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "train_labels = train_dataset.pop('class')\n",
        "test_labels = test_dataset.pop('class')\n"
      ],
      "metadata": {
        "id": "j8ActdUx1wzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertimos todo a arrays Numpy\n",
        "X_train = train_dataset.to_numpy()\n",
        "X_test = test_dataset.to_numpy()\n",
        "\n",
        "Y_train = train_labels.to_numpy()\n",
        "Y_test = test_labels.to_numpy()\n",
        "\n",
        "Y_train = Y_train[:,None]\n",
        "Y_test = Y_test[:,None]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "id": "EkVs8jnHg70j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parte 1**\n",
        "Diseña y entrena un perceptrón multicapa con la data de arriba. Intenta que tu modelo alcance el mayor accuracy de test posible (ojalá por encima del 93%). Para este primer experimento puedes usar la implementación de MLP vista en clase (basada en Numpy) ó puedes usar algún framework como Tensorflow o Pytorch si es que deseas. Hay que tener en cuenta algunas consideraciones para este primer experimento:\n",
        "\n",
        "\n",
        "\n",
        "*   Usar gradiente descendiente estocástico con un tamaño de mini-batch de 20.\n",
        "*   Usar learning rate de 0.01.\n",
        "*  Los mini-batches NO se generan de manera aleatoria.\n",
        "\n",
        "Graficar la función de loss con respecto a las épocas."
      ],
      "metadata": {
        "id": "k45gRQcSoQEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parte 2**\n",
        "Intenta cambiar el tamaño de los mini-batches. Como casos extremos usa m=1 (mini-batches de tamaño 1) y m=n(1 solo mini-batch con todos los datos). Qué resultados obtienes? Discute los resultados. (Hay que explicar el fenómeno observado, dando su opinión de porqué sucede).\n",
        "\n",
        "Graficar la función de loss con respecto a las épocas y comparar con la función de loss de la Parte 1."
      ],
      "metadata": {
        "id": "mJR12xjqpnu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parte 3**\n",
        "Intenta cambiar el learning rate. Como casos extremos usa lr = 0.5 y lr = 0.000001. Qué resultados obtienes? Discute los resultados. (Hay que explicar el fenómeno observado, dando su opinión de porqué sucede).\n",
        "\n",
        "Graficar la función de loss con respecto a las épocas y comparar con la función de loss de la Parte 1."
      ],
      "metadata": {
        "id": "ANcEAlvhqN27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parte 4**\n",
        "Implementar el algoritmo de gradiente descendente estocástico de manera que los mini-batches se generen de manera aleatoria antes de cada época. Qué resultados obtienes? Discute los resultados. (Hay que explicar el fenómeno observado, dando su opinión de porqué sucede).\n",
        "\n",
        "Graficar la función de loss con respecto a las épocas y comparar con la función de loss de la Parte 1."
      ],
      "metadata": {
        "id": "E9KM20PxqaHU"
      }
    }
  ]
}